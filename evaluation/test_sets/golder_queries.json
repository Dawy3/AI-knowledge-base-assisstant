[
  {
    "query_id": "q001",
    "query": "What is machine learning?",
    "expected_doc_ids": [0, 1, 2],
    "expected_answer": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing computer programs that can access data and use it to learn for themselves.",
    "category": "definition",
    "difficulty": "easy"
  },
  {
    "query_id": "q002",
    "query": "How does retrieval-augmented generation work?",
    "expected_doc_ids": [5, 6, 7],
    "expected_answer": "Retrieval-augmented generation (RAG) works by first retrieving relevant documents from a knowledge base, then using those documents as context for a language model to generate a response. This approach helps reduce hallucinations and provides more accurate, grounded responses.",
    "category": "process",
    "difficulty": "medium"
  },
  {
    "query_id": "q003",
    "query": "What are vector databases used for in AI applications?",
    "expected_doc_ids": [10, 11, 12],
    "expected_answer": "Vector databases are used in modern AI applications for semantic search, recommendation systems, and retrieval-augmented generation. They store and query high-dimensional vectors efficiently using techniques like HNSW graphs.",
    "category": "application",
    "difficulty": "medium"
  },
  {
    "query_id": "q004",
    "query": "Explain the difference between supervised and unsupervised learning",
    "expected_doc_ids": [15, 16, 17],
    "expected_answer": "Supervised learning uses labeled training data where the correct output is known, allowing the model to learn the mapping from inputs to outputs. Unsupervised learning works with unlabeled data, finding patterns and structure without predefined categories.",
    "category": "comparison",
    "difficulty": "medium"
  },
  {
    "query_id": "q005",
    "query": "What is the purpose of embedding normalization?",
    "expected_doc_ids": [20, 21],
    "expected_answer": "Embedding normalization (L2 normalization) ensures that all vectors have unit length, which makes cosine similarity equivalent to dot product and improves retrieval quality by focusing on direction rather than magnitude.",
    "category": "technical",
    "difficulty": "hard"
  },
  {
    "query_id": "q006",
    "query": "How does chunking improve RAG performance?",
    "expected_doc_ids": [25, 26, 27],
    "expected_answer": "Chunking breaks large documents into smaller, semantically coherent pieces that fit within context windows. This improves retrieval precision by matching specific relevant passages rather than entire documents, and allows for better embedding quality.",
    "category": "optimization",
    "difficulty": "medium"
  },
  {
    "query_id": "q007",
    "query": "What is the role of reranking in retrieval pipelines?",
    "expected_doc_ids": [30, 31],
    "expected_answer": "Reranking uses a more accurate but slower model (cross-encoder) to rescore the top candidates from initial retrieval. This two-stage approach balances speed and accuracy by using fast retrieval for recall and precise reranking for final ranking.",
    "category": "architecture",
    "difficulty": "hard"
  },
  {
    "query_id": "q008",
    "query": "Why use hybrid search instead of pure vector search?",
    "expected_doc_ids": [35, 36, 37],
    "expected_answer": "Hybrid search combines vector (semantic) and keyword (BM25) search to capture both meaning and exact term matches. This handles cases where semantic search alone might miss important keyword matches, improving overall recall and precision.",
    "category": "comparison",
    "difficulty": "medium"
  },
  {
    "query_id": "q009",
    "query": "What is HNSW and why is it used in vector databases?",
    "expected_doc_ids": [40, 41],
    "expected_answer": "HNSW (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor search. It provides fast retrieval with high recall by organizing vectors in a multi-layer graph structure, making it ideal for large-scale vector databases.",
    "category": "algorithm",
    "difficulty": "hard"
  },
  {
    "query_id": "q010",
    "query": "How does semantic caching reduce LLM costs?",
    "expected_doc_ids": [45, 46],
    "expected_answer": "Semantic caching stores responses for similar queries by comparing query embeddings. When a new query is semantically similar to a cached one (above threshold), the cached response is returned, avoiding expensive LLM API calls and retrieval.",
    "category": "optimization",
    "difficulty": "medium"
  },
  {
    "query_id": "q011",
    "query": "What metrics should be used to evaluate retrieval quality?",
    "expected_doc_ids": [50, 51, 52],
    "expected_answer": "Key retrieval metrics include Recall@K (percentage of relevant docs retrieved), Precision@K (percentage of retrieved docs that are relevant), MRR (Mean Reciprocal Rank), and NDCG (Normalized Discounted Cumulative Gain) for ranking quality.",
    "category": "evaluation",
    "difficulty": "medium"
  },
  {
    "query_id": "q012",
    "query": "Explain the concept of context window in LLMs",
    "expected_doc_ids": [55, 56],
    "expected_answer": "The context window is the maximum number of tokens (words/subwords) an LLM can process at once, including both input and output. It limits how much information can be provided as context for generation, making chunking and retrieval strategies critical.",
    "category": "concept",
    "difficulty": "easy"
  },
  {
    "query_id": "q013",
    "query": "What is the difference between bi-encoder and cross-encoder models?",
    "expected_doc_ids": [60, 61, 62],
    "expected_answer": "Bi-encoders encode query and documents separately, enabling fast retrieval through vector similarity. Cross-encoders process query-document pairs together for more accurate scoring but are slower. RAG systems typically use bi-encoders for retrieval and cross-encoders for reranking.",
    "category": "architecture",
    "difficulty": "hard"
  },
  {
    "query_id": "q014",
    "query": "How does BM25 ranking work?",
    "expected_doc_ids": [65, 66],
    "expected_answer": "BM25 (Best Match 25) is a probabilistic ranking function that scores documents based on term frequency and inverse document frequency, with saturation to prevent over-weighting of term frequency. It's the foundation of keyword-based search.",
    "category": "algorithm",
    "difficulty": "hard"
  },
  {
    "query_id": "q015",
    "query": "What are the benefits of query expansion in RAG?",
    "expected_doc_ids": [70, 71],
    "expected_answer": "Query expansion generates multiple variations of the original query to improve recall. This helps retrieve relevant documents that might use different terminology or phrasing than the user's query, addressing the vocabulary mismatch problem.",
    "category": "optimization",
    "difficulty": "medium"
  },
  {
    "query_id": "q016",
    "query": "Why is metadata filtering important in vector search?",
    "expected_doc_ids": [75, 76, 77],
    "expected_answer": "Metadata filtering allows constraining search to specific subsets (e.g., recent documents, specific categories) before or during vector similarity search. This improves relevance and reduces computation by eliminating irrelevant documents early.",
    "category": "feature",
    "difficulty": "medium"
  },
  {
    "query_id": "q017",
    "query": "What is the purpose of document prefixes in embeddings?",
    "expected_doc_ids": [80, 81],
    "expected_answer": "Document prefixes (like 'passage:' or 'document:') help the embedding model distinguish between queries and documents during training, leading to better quality embeddings that are optimized for asymmetric retrieval tasks.",
    "category": "technical",
    "difficulty": "hard"
  },
  {
    "query_id": "q018",
    "query": "How does temperature affect LLM generation?",
    "expected_doc_ids": [85, 86],
    "expected_answer": "Temperature controls randomness in LLM sampling. Lower values (e.g., 0.1) make outputs more deterministic and focused, while higher values (e.g., 0.9) increase diversity and creativity. For factual QA, lower temperatures are typically preferred.",
    "category": "generation",
    "difficulty": "easy"
  },
  {
    "query_id": "q019",
    "query": "What is the vocabulary mismatch problem in information retrieval?",
    "expected_doc_ids": [90, 91],
    "expected_answer": "Vocabulary mismatch occurs when users and documents use different words for the same concepts (e.g., 'car' vs 'automobile'). Semantic search with embeddings helps solve this by understanding meaning rather than just matching keywords.",
    "category": "problem",
    "difficulty": "medium"
  },
  {
    "query_id": "q020",
    "query": "Why is faithfulness important in RAG evaluation?",
    "expected_doc_ids": [95, 96],
    "expected_answer": "Faithfulness measures whether the generated answer is grounded in the retrieved context without hallucinations. High faithfulness (>90%) ensures the system doesn't make up information, which is critical for production reliability.",
    "category": "evaluation",
    "difficulty": "medium"
  }
]